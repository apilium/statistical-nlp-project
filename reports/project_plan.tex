\documentclass{article}

\usepackage{hyperref}

%Includes References in the table of contents%
\usepackage[nottoc]{tocbibind}

\usepackage[square, numbers]{natbib}
\bibliographystyle{abbrvnat}


\title{Team 26: Text summarization}


\author{Teemu Sormunen (teemu.t.sormunen@aalto.fi)\\
	Abdullah Gunay\\
	Vasileios Christoforidis (vasileios.christoforidis@aalto.fi)\\
}


\begin{document}
	
\maketitle

\begin{abstract}
\noindent
	In our project, we have decided to create text summarizer with BERT. 
	There is already a Bert Extractive Summarizer -package. It's based on following article: \href{https://arxiv.org/ftp/arxiv/papers/1906/1906.04165.pdf}{https://arxiv.org/ftp/arxiv/papers/1906/1906.04165.pdf}
	This package is intended for lecture-summarization, and our goal is to extend and fine-tune this model for news or scientific articles.
	Even though foundational work is already done for the package, there's still much to customize such as tokenizer and model. 
	Our target language is english. Summarization tools such as Bilingual Evaluation Understudy (BLEU) and Recall-Oriented Understudy for Gisting Evaluation (ROUGE) are available, and part of the project is to try to evaluate the texts automatically and manually.	
\end{abstract}

\clearpage
\section{The dataset}

The Cornell University NEWSROOM dataset \cite{dataset} consists of 1.3 million news articles and summaries between the yeras 1998 and 2017. It contains articles from 38 major publications and the summarization strategies combine both extraction and abstraction. The Newsroom website (summari.es) contains various different tools for analyzing the large article database. The dataset is \\

\noindent
NOTE-TO-SELF: \\
\textbf{Extractive summarization} generates verbatim summarization. It takes subset of the sentences in the original text, and attempts to identify important sections. \cite{dataset}\\
\textbf{Abstractive summarization} interprets the original text, and creates a new, shorter text that attempts to have the same amount of relevant information.\\

Written by humans with purpose of summarization.
Design new benchmark. \\

Document Understanding Conference: Can be used as test set. It has multiple reference summaries for each article. High quality.
Small data. 

Previously simulated summaries were used as training data, e.g. headlines. 

Other mentionable dataset is new york times corpus which is the largest summarization dataset currently available. All articles from The New York times. Several hunder thousand articles written between 1987-2007. Summaries written by library scientists. Biased towards extractive strategies.\\

Dataset was created by scraping the website for content and using summaries in the HTML metadata. These metadata summaries were created to be found from search engines and social medias. Top news websites gathered from Alexa.com were used. 

In the paper \cite{dataset} the dataset is also analysed to understand what kind of summarization techniques are used.


% Import bibliography file %
\bibliography{./citations/sources.bib}
	
\end{document}
\documentclass{article}

\usepackage{hyperref}

%Includes References in the table of contents%
\usepackage[nottoc]{tocbibind}

\usepackage[square, numbers]{natbib}
\bibliographystyle{abbrvnat}


\title{Team 26: Text summarization}


\author{Teemu Sormunen (teemu.t.sormunen@aalto.fi)\\
	Abdullah Gunay\\
	Vasileios Christoforidis (vasileios.christoforidis@aalto.fi)\\
}


\begin{document}
	
\maketitle

\begin{abstract}
\noindent
	In our project, we have decided to create text summarizer with BERT. 
	There is already a Bert Extractive Summarizer -package. It's based on following article: \href{https://arxiv.org/ftp/arxiv/papers/1906/1906.04165.pdf}{https://arxiv.org/ftp/arxiv/papers/1906/1906.04165.pdf}
	This package is intended for lecture-summarization, and our goal is to extend and fine-tune this model for news or scientific articles.
	Even though foundational work is already done for the package, there's still much to customize such as tokenizer and model. 
	Our target language is english. Summarization tools such as Bilingual Evaluation Understudy (BLEU) and Recall-Oriented Understudy for Gisting Evaluation (ROUGE) are available, and part of the project is to try to evaluate the texts automatically and manually.	
\end{abstract}

\clearpage
\section{The dataset}

The Cornell University NEWSROOM dataset \cite{dataset} consists of 1.3 million news articles and summaries between the yeras 1998 and 2017. It contains articles from 38 major publications and the summarization strategies combine both extraction and abstraction. The Newsroom website (summari.es) contains various different tools for analyzing the large article database. The dataset is \\

\noindent
NOTE-TO-SELF: \\
\textbf{Extractive summarization} generates verbatim summarization. It takes subset of the sentences in the original text, and attempts to identify important sections. \cite{dataset}\\
\textbf{Abstractive summarization} interprets the original text, and creates a new, shorter text that attempts to have the same amount of relevant information.\\
\textbf{Extractive Fragment Coverage} explains how many words in the summary are from the article. Many words from article in the summary means high coverage.\\
\textbf{Extractive fragment density} explains average length of extractive fragment. Coverage can be high, but if the consecutive words (fragments) are small, then it might present new information (be more abstractive). So high density means, that there's long fragments from original text.\\
\textbf{compression ratio} defines the proportion of article word amount and summary word amount. \\

Written by humans with purpose of summarization.
Design new benchmark. \\

Document Understanding Conference: Can be used as test set. It has multiple reference summaries for each article. High quality.
Small data. 

Previously simulated summaries were used as training data, e.g. headlines. 

Other mentionable dataset is new york times corpus which is the largest summarization dataset currently available. All articles from The New York times. Several hunder thousand articles written between 1987-2007. Summaries written by library scientists. Biased towards extractive strategies.\\

Dataset was created by scraping the website for content and using summaries in the HTML metadata. These metadata summaries were created to be found from search engines and social medias. Top news websites gathered from Alexa.com were used. 

In the paper \cite{dataset} the dataset is also analysed to understand what kind of summarization techniques are used.

\clearpage
\section{State of art }

Often, for the baseline precision Lede-3 is used. It's automatic summarization strategy that copies first $n$ words of the first sentence and uses it as summary. In our work, we will be using this as baseline also.

Current state of art models in text summarization can be classified to fully extractive, fully abstractive or mixed models. In the study \cite{dataset} ROUGE-1, ROUGE-2 and ROUGE-L $F_1$ score variants were used to account for different summary lengths. In our work we will also be using ROUGE scores, as BLEU score requires multiple human annotated references, and we only have 1.

In the leaderboard it seems that mixed models perform the best, then comes the extractive and abstractive is the least efficient.

% Import bibliography file %
\bibliography{./citations/sources.bib}
	
\end{document}
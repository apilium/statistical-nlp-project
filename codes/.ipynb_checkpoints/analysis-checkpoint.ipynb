{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import torch, transformers\n",
    "import nltk\n",
    "import sklearn\n",
    "import gzip, json"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Inspect memory usage"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                           _i2:  588.0 B\n",
      "                           _oh:  240.0 B\n",
      "                           Out:  240.0 B\n",
      "                            _i:  159.0 B\n",
      "                           _i1:  159.0 B\n",
      "                    sizeof_fmt:  136.0 B\n",
      "                       __doc__:  113.0 B\n",
      "                           _ih:   96.0 B\n",
      "                            In:   96.0 B\n",
      "                   __builtin__:   80.0 B\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "def sizeof_fmt(num, suffix='B'):\n",
    "    ''' by Fred Cirera,  https://stackoverflow.com/a/1094933/1870254, modified'''\n",
    "    for unit in ['','Ki','Mi','Gi','Ti','Pi','Ei','Zi']:\n",
    "        if abs(num) < 1024.0:\n",
    "            return \"%3.1f %s%s\" % (num, unit, suffix)\n",
    "        num /= 1024.0\n",
    "    return \"%.1f %s%s\" % (num, 'Yi', suffix)\n",
    "\n",
    "for name, size in sorted(((name, sys.getsizeof(value)) for name, value in globals().items()),\n",
    "                         key= lambda x: -x[1])[:10]:\n",
    "    print(\"{:>30}: {:>8}\".format(name, sizeof_fmt(size)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data pipeline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Specify paths"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Modify this to wherever you locally downloaded the data\n",
    "data_base_path = './data/newsroom-release/release/'\n",
    "wordpiece_cased_path = 'bert-base-cased-vocab.txt'\n",
    "\n",
    "# train_path = data_base_path + 'train.jsonl.gz' DONT USE THIS\n",
    "validation_path = data_base_path + 'dev.jsonl.gz'\n",
    "test_path = data_base_path + 'dev.jsonl.gz'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class NewsroomDataset(torch.utils.data.Dataset):\n",
    "    '''\n",
    "    Attributes:\n",
    "        batch_size: Batch size to be taken on single getitem\n",
    "        file: path to the dataset file\n",
    "        category: category of the data summarization. i.e. 'extractive'\n",
    "    '''\n",
    "    def __init__(self, path, category: str):\n",
    "        self.category = category\n",
    "        data = []\n",
    "        with gzip.open(path) as f:\n",
    "            for ln in f:\n",
    "                obj = json.loads(ln)\n",
    "                data.append(obj)\n",
    "        data = pd.DataFrame(data)\n",
    "        # Take only samples with certain category\n",
    "        self.data = data.loc[data['density_bin'] == self.category, :]\n",
    "        \n",
    "    def __len__(self):\n",
    "        return len(self.data)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        return dict(self.data.iloc[idx, :])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Dataset is too big to load to memory - create minibatches and parallelize loading with DataLoader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_dset = NewsroomDataset(test_path, \"extractive\")\n",
    "testloader = torch.utils.data.DataLoader(test_dset, batch_size=2)    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Get batch for visualization"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Tokenization"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Import the dependencies and initialize tokenizer and model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import BertTokenizer, BertModel\n",
    "from tokenizers import (BertWordPieceTokenizer)\n",
    "from sklearn.cluster import KMeans\n",
    "from operator import itemgetter\n",
    "tokenizer = nltk.data.load('tokenizers/punkt/english.pickle')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "bert_model = 'bert-base-cased'\n",
    "bert_tokenizer = BertTokenizer.from_pretrained(bert_model)\n",
    "bert_wordpiece_tokenizer = BertWordPieceTokenizer(wordpiece_cased_path)\n",
    "bert_base_model = BertModel.from_pretrained(bert_model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Split text to sentences, and tokenize sentences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tokenize_sentences(sentences):\n",
    "    \"\"\"Tokenizes and preprocesses to sentences\"\"\"\n",
    "    # Split to sentences, Remove too short sentences\n",
    "    sents = tokenizer.sent_tokenize(sentences)\n",
    "    sents = [bert_wordpiece_tokenizer.encode(s).tokens for s in sents] \n",
    "    return sents\n",
    "\n",
    "def best_n_summaries(centroids, pooler_outputs, model_outputs: np.ndarray, n_summaries=1):\n",
    "    '''\n",
    "    Returns n most likely sentences for summarization\n",
    "    \n",
    "    Attributes:\n",
    "        n_summaries: How many sentences to choose from a single centroid (for debugging)\n",
    "        centroids: Centroids of the K-clusters\n",
    "        pooler_outputs: embeddings from the model's [CLS] token\n",
    "        model_outputs: Text tokenized to sentences. Used for retrieving sentences from embeddings\n",
    "    '''\n",
    "    assert n_summaries < pooler_outputs.shape[0], \"n_summaries must be less than sentences in the trainset\"\n",
    "    \n",
    "    summarizations = []\n",
    "        \n",
    "    for k in centroids: # Go through centroids\n",
    "        indices = [] # Get indices for each pooler output\n",
    "        values = [] # Get distances to the centroid k\n",
    "        \n",
    "        # Go through pooler outputs, and find sentence closest to centroid\n",
    "        for i, sample in enumerate(pooler_outputs):\n",
    "            current_dist = np.linalg.norm(sample - k)\n",
    "            values.append(current_dist)\n",
    "            indices.append(i)\n",
    "\n",
    "        # Sort indices based on values\n",
    "        _, indices = zip(*sorted(zip(values, indices)))\n",
    "        indices = list(indices)\n",
    "        \n",
    "        summarization = model_outputs[indices[:n_summaries]]\n",
    "        summarizations.append(''.join(summarization))\n",
    "        \n",
    "    return summarizations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "ename": "SyntaxError",
     "evalue": "invalid syntax (<ipython-input-9-51be42eddef0>, line 1)",
     "output_type": "error",
     "traceback": [
      "\u001b[1;36m  File \u001b[1;32m\"<ipython-input-9-51be42eddef0>\"\u001b[1;36m, line \u001b[1;32m1\u001b[0m\n\u001b[1;33m    bert_wordpiece_tokenizer.encode(x).tokens for x in text_tokens[0]\u001b[0m\n\u001b[1;37m                                                ^\u001b[0m\n\u001b[1;31mSyntaxError\u001b[0m\u001b[1;31m:\u001b[0m invalid syntax\n"
     ]
    }
   ],
   "source": [
    "bert_wordpiece_tokenizer.encode(x).tokens for x in text_tokens[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Test metrics"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Import dependencies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "from rouge import Rouge"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create prediction loop, test rouge score for each batch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "too many values to unpack (expected 2)",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-11-cf36feda7a80>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     20\u001b[0m                                          \u001b[0mmax_length\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mMAX_LENGTH\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     21\u001b[0m                                          \u001b[0mreturn_tensors\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m'pt'\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 22\u001b[1;33m                                          padding='max_length')['input_ids']\n\u001b[0m\u001b[0;32m     23\u001b[0m         \u001b[0mtext_embeddings\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcat\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtext_embeddings\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0membeds\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     24\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\envs\\pytorchEnv\\lib\\site-packages\\transformers\\tokenization_utils_base.py\u001b[0m in \u001b[0;36m__call__\u001b[1;34m(self, text, text_pair, add_special_tokens, padding, truncation, max_length, stride, is_split_into_words, pad_to_multiple_of, return_tensors, return_token_type_ids, return_attention_mask, return_overflowing_tokens, return_special_tokens_mask, return_offsets_mapping, return_length, verbose, **kwargs)\u001b[0m\n\u001b[0;32m   2264\u001b[0m                 \u001b[0mreturn_length\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mreturn_length\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2265\u001b[0m                 \u001b[0mverbose\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mverbose\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 2266\u001b[1;33m                 \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   2267\u001b[0m             )\n\u001b[0;32m   2268\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\envs\\pytorchEnv\\lib\\site-packages\\transformers\\tokenization_utils_base.py\u001b[0m in \u001b[0;36mbatch_encode_plus\u001b[1;34m(self, batch_text_or_text_pairs, add_special_tokens, padding, truncation, max_length, stride, is_split_into_words, pad_to_multiple_of, return_tensors, return_token_type_ids, return_attention_mask, return_overflowing_tokens, return_special_tokens_mask, return_offsets_mapping, return_length, verbose, **kwargs)\u001b[0m\n\u001b[0;32m   2449\u001b[0m             \u001b[0mreturn_length\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mreturn_length\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2450\u001b[0m             \u001b[0mverbose\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mverbose\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 2451\u001b[1;33m             \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   2452\u001b[0m         )\n\u001b[0;32m   2453\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\envs\\pytorchEnv\\lib\\site-packages\\transformers\\tokenization_utils.py\u001b[0m in \u001b[0;36m_batch_encode_plus\u001b[1;34m(self, batch_text_or_text_pairs, add_special_tokens, padding_strategy, truncation_strategy, max_length, stride, is_split_into_words, pad_to_multiple_of, return_tensors, return_token_type_ids, return_attention_mask, return_overflowing_tokens, return_special_tokens_mask, return_offsets_mapping, return_length, verbose, **kwargs)\u001b[0m\n\u001b[0;32m    521\u001b[0m                 \u001b[0mids\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mpair_ids\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mids_or_pair_ids\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    522\u001b[0m             \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 523\u001b[1;33m                 \u001b[0mids\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mpair_ids\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mids_or_pair_ids\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    524\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    525\u001b[0m             \u001b[0mfirst_ids\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mget_input_ids\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mids\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mValueError\u001b[0m: too many values to unpack (expected 2)"
     ]
    }
   ],
   "source": [
    "rouge = Rouge()\n",
    "scores = []\n",
    "MAX_LENGTH = 128 # Pad to this length\n",
    "bert_base_model.eval()\n",
    "\n",
    "for i, batch in enumerate(testloader):\n",
    "    ### TODO ###\n",
    "    # currently we only get single sample from batch, we need better indxing from the batch\n",
    "    b = batch\n",
    "    # Get text and summary from batch\n",
    "    text, summary = itemgetter('text', 'summary')(batch)\n",
    "    # Preprocess\n",
    "    summary_tokens = [tokenize_sentences(s) for s in summary]\n",
    "    text_tokens = [tokenize_sentences(t) for t in text]\n",
    "    \n",
    "    # Create empty tensor, append new embeddings\n",
    "    text_embeddings = torch.zeros((1, MAX_LENGTH), dtype=torch.int)\n",
    "    for text_token in text_tokens:\n",
    "        embeds = bert_tokenizer(text=text_token, \n",
    "                                         max_length=MAX_LENGTH,\n",
    "                                         return_tensors='pt', \n",
    "                                         padding='max_length')['input_ids']\n",
    "        text_embeddings = torch.cat((text_embeddings, embeds))\n",
    "    \n",
    "    # Create embeddings\n",
    "    print(f\"text_embeddings: {text_embeddings.shape}\")\n",
    "    print(text_embeddings)\n",
    "    with torch.no_grad():\n",
    "        model_out = bert_base_model(text_embeddings)\n",
    "    \n",
    "    embeddings = model_out.pooler_output.detach().numpy()\n",
    "    \n",
    "    print(f\"embeddings: {embeddings}\")\n",
    "    # Do K-means clustering\n",
    "    k = len(summary_tokens)\n",
    "    kmeans = KMeans(n_clusters=k).fit(embeddings)\n",
    "    # Find corresponding summary sentences\n",
    "    centroids = kmeans.cluster_centers_\n",
    "    text_summaries = best_n_summaries(centroids=centroids, \n",
    "                                      pooler_outputs=embeddings, \n",
    "                                      model_outputs=np.array(text_tokens))\n",
    "    \n",
    "    # Evaluation\n",
    "    print(f\"text_summaries: {text_summaries}\")\n",
    "    score = rouge.get_scores(text_summaries, summary_tokens)\n",
    "    scores.append(score)\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'BANGALORE, India, June 4 \\x97 The world\\'s biggest computer services company could not have chosen a more appropriate setting to lay out its strategy for staying on top.\\n\\nA building housing I.B.M.\\'s software laboratory and application service teams on the company\\'s corporate campus in Bangalore, India.\\n\\nOn Tuesday, on the expansive grounds of the Bangalore Palace, a colonial-era mansion once inhabited by a maharajah, the chairman and chief executive of I.B.M., Samuel J. Palmisano, will address 10,000 Indian employees. He will share the stage with A. P. J. Abdul Kalam, India\\'s president, and Sunil Mittal, chairman of the country\\'s largest cellular services provider, Bharti Tele-Ventures. An additional 6,500 employees will look in on the town hall-style meeting by satellite from other Indian cities.\\n\\nOn the same day, Mr. Palmisano and other top executives will meet here with investment analysts and local customers to showcase I.B.M.\\'s global integration capabilities in a briefing customarily held in New York. During the week, the company will lead the 50 analysts on a tour of its Indian operations.\\n\\nThe meetings are more than an exercise in public and investor relations. They are an acknowledgment of India\\'s critical role in I.B.M.\\'s strategy, providing it with its fastest-growing market and a crucial base for delivering services to much of the world.\\n\\n\"A significant part of any large project that we do worldwide is today being delivered out of here,\" said Shanker Annaswamy, I.B.M.\\'s managing director for India, who presides over what is now the company\\'s second-largest worldwide operation. In the last few years, even as the company has laid off thousands of workers in the United States and Europe, the growth in I.B.M.\\'s work force in India has been remarkable. From 9,000 employees in early 2004, the number has grown to 43,000 (out of 329,000 worldwide), making I.B.M. the country\\'s largest multinational employer.\\n\\nSome of the growth has been through acquisition. In a deal valued at about $160 million in 2004, I.B.M. bought Daksh eServices of New Delhi, India\\'s third-largest back-office outsourcing firm with 6,000 workers. Since then, that operation alone has grown to 20,000 employees.\\n\\n\"Now that companies such as Infosys Technologies and Cognizant have clearly demonstrated that the services marketplace is not impregnable, the new battle is for talent,\" said N. Lakshmi Narayanan, president and chief executive of Cognizant Technology Solutions of Teaneck, N.J. Cognizant is one of I.B.M.\\'s competitors; it is incorporated in the United States but has the bulk of its 28,000 employees in India.\\n\\nI.B.M. is growing not only in size by adding new hires, but also in revenue. The company\\'s business in India grew 61 percent in the first quarter of this year, 55 percent in 2005 and 45 percent the year before.\\n\\nThat growth has not come just from taking advantage of the country\\'s pool of low-cost talent. In recent months, the technology hub of Bangalore has become the center of I.B.M.\\'s efforts to combine high-value, cutting-edge services with its low-cost model.\\n\\nFor instance, the I.B.M. India Research Lab, with units in Bangalore and New Delhi and a hundred employees with Ph.D.\\'s, has created crucial products like a container tracking system for global shipping companies and a warranty management system for automakers in the United States. Out of the second project, I.B.M. researchers have fashioned a predictable modeling system that helps track the failure of components inside a vehicle, a potentially important tool.\\n\\nIn March, the company started a Global Business Solutions Center here, announcing that it would represent the \"future of consulting services.\" I.B.M. said that it expected to invest more than $200 million a year in the new center. The company hopes to provide clients with access to the expertise of its 60,000 consultants worldwide in complex areas like supply chain management and compliance with banking rules.\\n\\nBut competitors are trying to gain on I.B.M. The rival consulting firm, Accenture, based in Hamilton, Bermuda, is ramping up equally rapidly in India, while another outsourcing competitor, Electronic Data Systems, based in Plano, Tex., recently made an offer for a controlling stake in Mphasis, a midsize outsourcing firm in Bangalore.\\n\\nThe race for India\\'s skilled, inexpensive talent may not stop at I.B.M. \"Many companies in the technology development and support niche covet and value these workers highly,\" said Kevin M. Moss, a New York-based special counsel in Kramer Levin Naftalis & Frankel\\'s outsourcing and technology transactions group.\\n\\nOn the pricing front, rivals like Tata Consultancy Services of Mumbai and Infosys Technologies of Bangalore have pioneered and perfected the low-cost model. Infosys Technologies, with 52,700 employees, has $2.15 billion in annual revenues, a figure that is growing 30 percent annually.\\n\\nBut the depth, breadth and geographic spread of I.B.M.\\'s global operations \\x97 which generated $91 billion in sales last year, $47 billion from services \\x97 keep it ahead of its competitors for now. For example, I.B.M. manages a system it developed for a large American oil company, which it would not identify, that keeps track of consumption and oversees financial and administrative processes as well as the technical help desk, data network and servers. I.B.M. is also researching tools to track company assets and reduce costs.\\n\\n\"All this is done for one customer seamlessly from three of our centers in Bangalore, Chicago and outside of London,\" said Amitabh Ray, director of global delivery, I.B.M. Global Services. \"These kinds of capabilities and global scale are unmatched.\"\\n\\nBut smaller rivals are playing catch-up here, too, by talking to customers about their needs and then developing custom-built software. Infosys Technologies, for instance, has a consulting unit with headquarters in Fremont, Calif., near Silicon Valley, where it now has 200 consultants, and an additional 1,800 consultants in India.\\n\\nMeanwhile, Mr. Annaswamy, I.B.M.\\'s chief executive in India, acknowledged that growth was difficult because thousands of recruits had to be quickly integrated into the company. Salaries are rising, and employee costs are also moving up, he said.\\n\\nEven so, the Indian operation is becoming more and more strategic for the company. \"Both in terms of size and scale, India has become the focal point,\" Mr. Ray, of I.B.M. Global Services, said.'"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "text[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "[t for t in text]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "e = torch.zeros((1, 128))\n",
    "o = bert_tokenizer(text=text_tokens[1], return_tensors='pt', max_length=128, padding='max_length', return_length=True)['input_ids']\n",
    "e = torch.cat((e, o))\n",
    "o = bert_tokenizer(text=text_tokens[0], return_tensors='pt', max_length=128, padding='max_length', return_length=True)['input_ids']\n",
    "torch.cat((e, o)).shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Test the K-Means clustering with single sample"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sample_train, sample_test = itemgetter('text', 'summary')(next(iter(trainloader)))\n",
    "\n",
    "sample_sentences = tokenize_sentences(sample_train)\n",
    "\n",
    "# Tokenize\n",
    "sample_train_tokens = bert_tokenizer(sample_sentences, return_tensors='pt', padding='longest')\n",
    "\n",
    "# Preprocess\n",
    "sample_sentences = np.array(sample_sentences)\n",
    "\n",
    "## Get BERT CLS embeddings\n",
    "model_output = bert_base_model(**sample_train_tokens)\n",
    "pooler_output = model_output.pooler_output.detach().numpy() # Get numpy array\n",
    "\n",
    "# Cluster embeddings to find centroids\n",
    "sample_sentences_test = tokenize_sentences(sample_test)\n",
    "\n",
    "# Choose same amount of centroids than the actual summary has\n",
    "k = len(sample_sentences_test)\n",
    "kmeans = KMeans(n_clusters=k).fit(pooler_output)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Find closest token to the centroid"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "# centroid = kmeans.cluster_centers_\n",
    "# summaries = best_n_summaries(3, centroid, pooler_output, sample_sentences)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Compare found centroid and actual summarization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def print_top_n_summaries(summaries, test_summary):\n",
    "    print(\"Generated summaries: \\n\")\n",
    "    [print(f\"{i+1}. {summary}\") for i, summary in enumerate(summaries)]\n",
    "    print(f\"\\n\\nActual summary: \\n {test_summary}\")\n",
    "    \n",
    "print_top_n_summaries(summaries, sample_sentences_test)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "pytorch",
   "language": "python",
   "name": "pytorch"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}

{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import torch, transformers\n",
    "import nltk\n",
    "import sklearn\n",
    "import gzip, json"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data pipeline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Specify paths"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Modify this to wherever you locally downloaded the data\n",
    "data_base_path = './data/newsroom-release/release/'\n",
    "\n",
    "train_path = data_base_path + 'train.jsonl.gz'\n",
    "validation_path = data_base_path + 'dev.jsonl.gz'\n",
    "test_path = data_base_path + 'dev.jsonl.gz'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate length\n",
    "with gzip.open(train_path, \"rb\") as f:\n",
    "    for i, x in enumerate(f):\n",
    "        pass\n",
    "dset_len = i"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class NewsroomDataset(torch.utils.data.Dataset):\n",
    "    '''\n",
    "    Attributes:\n",
    "        batch_size: Batch size to be taken on single getitem\n",
    "        file: path to the dataset file\n",
    "        category: category of the data summarization. i.e. 'extractive'\n",
    "    '''\n",
    "    def __init__(self, path, category: str, dataset_len: int):\n",
    "        self.file = gzip.open(path, \"rt\")\n",
    "        self.category = category\n",
    "        self.length = dataset_len\n",
    "        \n",
    "    def __len__(self):\n",
    "        return self.length\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        return self.load_data()\n",
    "\n",
    "    \n",
    "    def load_data(self):\n",
    "        # Find sample that is in our category\n",
    "        sample = json.loads(self.file.readline())\n",
    "        while sample['density_bin'] != self.category:\n",
    "            sample = json.loads(self.file.readline())\n",
    "\n",
    "        return sample"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Dataset is too big to load to memory - create minibatches and parallelize loading with DataLoader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dset = NewsroomDataset(train_path, \"extractive\", dataset_len=dset_len)\n",
    "trainloader = torch.utils.data.DataLoader(train_dset, batch_size=2)    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Get batch for visualization"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Tokenization"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Import the dependencies and initialize tokenizer and model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\teemu\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from transformers import BertTokenizer, BertModel\n",
    "from nltk import tokenize\n",
    "from sklearn.cluster import KMeans\n",
    "from operator import itemgetter\n",
    "nltk.download('punkt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "bert_model = 'bert-base-cased'\n",
    "bert_tokenizer = BertTokenizer.from_pretrained(bert_model)\n",
    "bert_base_model = BertModel.from_pretrained(bert_model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Split text to sentences, and tokenize sentences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tokenize_sentences(sentences):\n",
    "    \"\"\"Tokenizes and preprocesses to sentences\"\"\"\n",
    "    sentences = [tokenize.sent_tokenize(s) for s in sentences][0] # Split to sentences\n",
    "    sentences = [x for x in sentences if len(x) > 2] # Remove too short sentences\n",
    "    return sentences"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Test metrics"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Import dependencies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "from rouge import Rouge"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create prediction loop, test rouge score for each batch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'url': ['http://www.cnbc.com/2013/12/09/hard-to-say-if-airline-consolidation-will-pause-american-airlines-ceo.html', 'http://www.cnbc.com/2014/09/10/beware-wild-cards-to-watch-for-in-2015.html'], 'archive': ['http://web.archive.org/web/20151102042147id_/http://www.cnbc.com/2013/12/09/hard-to-say-if-airline-consolidation-will-pause-american-airlines-ceo.html', 'http://web.archive.org/web/20151103051855id_/http://www.cnbc.com/2014/09/10/beware-wild-cards-to-watch-for-in-2015.html'], 'title': ['American Airlines CEO', 'Beware: Wild cards to watch for in 2015'], 'date': ['20151102042147', '20151103051855'], 'text': ['\"The three of us have now the ability to take people pretty much anywhere in the world. What used to be a business where it was purely on schedule, if you have the ability to take people everywhere, you have to compete on product. And we\\'re prepared to do that,\" said Parker, who had been the CEO of US Airways before the merger.\\n\\nThe newly combined carrier began trading Monday morning under the ticker symbol AAL.\\n\\nWith American Airlines coming out of bankruptcy with this merger, Parker said, airlines that aren\\'t profitable won\\'t be able to grow. \"We expect to produce a profit that will provide a nice return for our investors. That\\'s what they expect. And that\\'s what we plan to deliver.\"\\n\\nThe new company\\'s modern fleet will give it a competitive edge, he continued, adding that nothing about this transaction will affect air fares because the carriers are highly complementary. \"We\\'re keeping all the airplanes, keeping all the people. So supply should be unchanged. As long as demand stays the same, nothing should happen to prices.\"', 'On top of the threat of the U.S. Federal Reserve tightening the screws, the risk of another recession in Europe and a stumbling recovery in Japan, there are a deck of \"wild cards\" that could trip up the global economy in the coming year, experts warned at the World Economic Forum (WEF) in Tianjin, China.\\n\\nCNBC shuffles through the pack to see what the world should look out for in 2015.\\n\\nA major, under-appreciated risk to the world economy is infectious diseases, says Victor Chu, chairman of Hong Kong based private equity firm First Eastern Investment Group.\\n\\n\"We\\'re not prepared for another major outbreak of Ebola, SARS or another infectious disease,\" Chu told CNBC on the sidelines of WEF in Tianjin, China.\\n\\nRead MoreGates Foundations pledges $50M to fight Ebola epidemic\\n\\n\"We need a new global governance structure to allow more experimental drugs to be distributed a lot more quickly than existing arrangements because you don\\'t know what\\'s next. Ebola is scary, but the next variation will be scarier,\" he added.\\n\\nEbola – which was declared an international public health emergency early last month – is already having a serious economic impact on the economies of Guinea, Liberia and Sierra Leone in West Africa, where the outbreak is occurring.\\n\\nThe closure of borders and suspension of flights has begun to hinder trade flows in the region, while tourism is also taking a severe hit.'], 'summary': ['CEO Doug Parker tells CNBC that \"as long as demand stays the same, nothing should happen to prices.\"', 'There are a deck of \"wild cards\" that could trip up the global economy in the coming year, experts warned at the World Economic Forum in Tianjin, China.'], 'compression': tensor([9.8182, 8.4545], dtype=torch.float64), 'coverage': tensor([0.8636, 1.0000], dtype=torch.float64), 'density': tensor([10.4091, 24.5152], dtype=torch.float64), 'compression_bin': ['low', 'low'], 'coverage_bin': ['medium', 'high'], 'density_bin': ['extractive', 'extractive']}\n"
     ]
    }
   ],
   "source": [
    "rouge = Rouge()\n",
    "scores = []\n",
    "\n",
    "for i, batch in enumerate(trainloader):\n",
    "    ### TODO ###\n",
    "    # currently we only get single sample from batch, we need better indxing from the batch\n",
    "    \n",
    "    # Get text and summary from batch\n",
    "    text, summary = itemgetter('text', 'summary')(batch)\n",
    "    # Preprocess\n",
    "    summary_tokens = tokenize_sentences(summary)\n",
    "    text_tokens = tokenize_sentences(text)\n",
    "    text_embeddings = bert_tokenizer(text_tokens, return_tensors='pt', padding='longest')['input_ids']\n",
    "    \n",
    "    # Create embeddings\n",
    "    print(f\"text_embeddings: {text_embeddings.shape}\")\n",
    "    model_out = bert_base_model(text_embeddings)\n",
    "    embeddings = model_out.pooler_output.detach().numpy()\n",
    "    \n",
    "    print(f\"embeddings: {embeddings}\")\n",
    "    # Do K-means clustering\n",
    "    k = len(summary_tokens)\n",
    "    kmeans = KMeans(n_clusters=k).fit(embeddings)\n",
    "    # Find corresponding summary sentences\n",
    "    centroids = kmeans.cluster_centers_\n",
    "    text_summaries = best_n_summaries(centroids=centroids, \n",
    "                                      pooler_outputs=embeddings, \n",
    "                                      model_outputs=np.array(text_tokens))\n",
    "    \n",
    "    # Evaluation\n",
    "    print(f\"text_summaries: {text_summaries}\")\n",
    "    score = rouge.get_scores(text_summaries, summary_tokens)\n",
    "    scores.append(score)\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[\"POPEYE-WORTHY PIE. PHYLLO DOUGH WRAPS SPINACH IN PURE GOLD BY ROSEMARY BLACK Spinach has terrorized generations of veggie-phobic kids, and many grownups don't much like it, either. But when it's combined with seasonings and feta cheese and wrapped in a golden crisp phyllo dough crust, even those who despise Popeye's Â\\xadfavorite food ask for seconds. The spinach pie at Kebab House II on Orchard St. is a specialty of owner and executive chef Ramazan Ay,\",\n",
       " 'All day, every day, Cheryl Bernstein thanks her 16-month-old son. \"I gave life to Reid, but he gave me life - a reason to get clean and go on,\"she said yesterday after graduating from the Manhattan Family Treatment Court program. Bernstein, 41, and her husband, Doug Flaumenbaum, 33, both recovering crack and heroin addicts, were among three dozen men and women who regained custody of their children.']"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "summary"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Test the K-Means clustering with single sample"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sample_train, sample_test = itemgetter('text', 'summary')(next(iter(trainloader)))\n",
    "\n",
    "sample_sentences = tokenize_sentences(sample_train)\n",
    "\n",
    "# Tokenize\n",
    "sample_train_tokens = bert_tokenizer(sample_sentences, return_tensors='pt', padding='longest')\n",
    "\n",
    "# Preprocess\n",
    "sample_sentences = np.array(sample_sentences)\n",
    "\n",
    "## Get BERT CLS embeddings\n",
    "model_output = bert_base_model(**sample_train_tokens)\n",
    "pooler_output = model_output.pooler_output.detach().numpy() # Get numpy array\n",
    "\n",
    "# Cluster embeddings to find centroids\n",
    "sample_sentences_test = tokenize_sentences(sample_test)\n",
    "\n",
    "# Choose same amount of centroids than the actual summary has\n",
    "k = len(sample_sentences_test)\n",
    "kmeans = KMeans(n_clusters=k).fit(pooler_output)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Find closest token to the centroid"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def best_n_summaries(centroids, pooler_outputs, model_outputs: np.ndarray, n_summaries=1):\n",
    "    '''\n",
    "    Returns n most likely sentences for summarization\n",
    "    \n",
    "    Attributes:\n",
    "        n_summaries: How many sentences to choose from a single centroid (for debugging)\n",
    "        centroids: Centroids of the K-clusters\n",
    "        pooler_outputs: embeddings from the model's [CLS] token\n",
    "        model_outputs: Text tokenized to sentences. Used for retrieving sentences from embeddings\n",
    "    '''\n",
    "    assert n_summaries < pooler_outputs.shape[0], \"n_summaries must be less than sentences in the trainset\"\n",
    "    \n",
    "    summarizations = []\n",
    "        \n",
    "    for k in centroids: # Go through centroids\n",
    "        indices = [] # Get indices for each pooler output\n",
    "        values = [] # Get distances to the centroid k\n",
    "        \n",
    "        # Go through pooler outputs, and find sentence closest to centroid\n",
    "        for i, sample in enumerate(pooler_outputs):\n",
    "            current_dist = np.linalg.norm(sample - k)\n",
    "            values.append(current_dist)\n",
    "            indices.append(i)\n",
    "\n",
    "        # Sort indices based on values\n",
    "        _, indices = zip(*sorted(zip(values, indices)))\n",
    "        indices = list(indices)\n",
    "        \n",
    "        summarization = model_outputs[indices[:n_summaries]]\n",
    "        summarizations.append(''.join(summarization))\n",
    "        \n",
    "    return summarizations\n",
    "\n",
    "# centroid = kmeans.cluster_centers_\n",
    "# summaries = best_n_summaries(3, centroid, pooler_output, sample_sentences)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Compare found centroid and actual summarization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def print_top_n_summaries(summaries, test_summary):\n",
    "    print(\"Generated summaries: \\n\")\n",
    "    [print(f\"{i+1}. {summary}\") for i, summary in enumerate(summaries)]\n",
    "    print(f\"\\n\\nActual summary: \\n {test_summary}\")\n",
    "    \n",
    "print_top_n_summaries(summaries, sample_sentences_test)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "pytorch",
   "language": "python",
   "name": "pytorch"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
